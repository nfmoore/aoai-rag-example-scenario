{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Scenario 1: Pull-Based Data Ingestion with Azure AI Search & Custom Retrieval-Augmented Generation (RAG) Pattern\n",
    "\n",
    "- **Populate Azure AI Search Index**: A pull-based approach is used to create a search index in Azure AI Search. A pull model uses indexers connecting to a supported data source, automatically uploading the data into your index. This is the recommended approach for data sources that are frequently updated.\n",
    "\n",
    "- **LLM Queries with Knowledge Base Integration**: A custom implementation for Retrieval Augmented Generation (RAG) will be used to chat with an LLM. This approach will be contrasted with an out-of-the box approach using the Azure OpenAI Service REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import sys\n",
    "\n",
    "# common setup\n",
    "dotenv.load_dotenv(\".env\")\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Populate Azure AI Search Index\n",
    "\n",
    "### Approach: Pull-Based Custom Client\n",
    "\n",
    "This approach will use the `CustomSearchClient` class defined in `search/custom_search_client_pull.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.custom_search_client import CustomSearchClient\n",
    "\n",
    "# Create search client\n",
    "search_client = CustomSearchClient(\n",
    "    search_endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of variables to be used in templates\n",
    "template_variables = {\n",
    "    key: value for key, value in os.environ.items() if key.startswith((\"AZURE\"))\n",
    "}\n",
    "\n",
    "# Define template paths\n",
    "base_path = os.path.join(os.getcwd(), \"..\", \"search\", \"templates\")\n",
    "datasource_template_path = os.path.join(base_path, \"product-info\", \"datasource.json\")\n",
    "index_template_path = os.path.join(base_path, \"product-info\", \"index.json\")\n",
    "skillset_template_path = os.path.join(base_path, \"product-info\", \"skillset.json\")\n",
    "indexer_template_path = os.path.join(base_path, \"product-info\", \"indexer.json\")\n",
    "\n",
    "# List of search assets\n",
    "assets = [\n",
    "    {\n",
    "        \"type\": \"indexes\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_INDEX_NAME\"],\n",
    "        \"template_path\": index_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"datasources\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_DATASOURCE_NAME\"],\n",
    "        \"template_path\": datasource_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"skillsets\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_SKILLSET_NAME\"],\n",
    "        \"template_path\": skillset_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"indexers\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_INDEXER_NAME\"],\n",
    "        \"template_path\": indexer_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load search asset templates\n",
    "search_client.load_search_management_asset_templates(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "index_response = search_client.create_search_management_asset(asset_type=\"indexes\")\n",
    "\n",
    "# Create the data source\n",
    "datasource_response = search_client.create_search_management_asset(\n",
    "    asset_type=\"datasources\"\n",
    ")\n",
    "\n",
    "# Create skillset to enhance the indexer\n",
    "skillset_response = search_client.create_search_management_asset(asset_type=\"skillsets\")\n",
    "\n",
    "# Create the indexer\n",
    "indexer_response = search_client.create_search_management_asset(asset_type=\"indexers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the indexer\n",
    "indexer_run_response = search_client.run_indexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] Run the indexer with reset\n",
    "# indexer_run_reset_response = search_client.run_indexer(reset_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Queries with Knowledge Base Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import sys\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Custom Client\n",
    "\n",
    "This approach will use the `CustomRetrievalAugmentedGenerationClient` class defined in `open_ai/custom_rag_client.py`. This will NOT require a Microsoft managed private endpoint for private access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration file\n",
    "system_prompt_configuration_file = \"../open_ai/configuration.yml\"\n",
    "with open(system_prompt_configuration_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    configuration = yaml.safe_load(f)\n",
    "\n",
    "# Get system messages\n",
    "query_system_message = configuration.get(\"query_system_message\")\n",
    "chat_system_message = configuration.get(\"product_info_chat_system_message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_ai.custom_rag_client import CustomRetrievalAugmentedGenerationClient\n",
    "\n",
    "# Create orchestration client\n",
    "rag_client = CustomRetrievalAugmentedGenerationClient(\n",
    "    open_ai_endpoint=os.getenv(\"AZURE_OPENAI_API_BASE\"),\n",
    "    open_ai_chat_deployment=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
    "    open_ai_embedding_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    search_endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    search_index_name=os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"),\n",
    "    query_system_message=query_system_message,\n",
    "    chat_system_message=chat_system_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = []\n",
    "message_history = rag_client.get_answer(\n",
    "    \"Which tent is the most waterproof?\", message_history=message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Which tent is the most waterproof?\n",
      "\n",
      "Assistant: Based on the information provided, the most waterproof tent is the Alpine Explorer Tent [product_info_8.md]. It provides reliable protection against rain and moisture.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in message_history:\n",
    "    print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = rag_client.get_answer(\n",
    "    \"Tell me more about the Alpine Explorer Tent?\", message_history=message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Which tent is the most waterproof?\n",
      "\n",
      "Assistant: Based on the information provided, the most waterproof tent is the Alpine Explorer Tent [product_info_8.md]. It provides reliable protection against rain and moisture.\n",
      "\n",
      "User: Tell me more about the Alpine Explorer Tent?\n",
      "\n",
      "Assistant: The Alpine Explorer Tent is a reliable and convenient tent for camping. Here are some key features and information about the tent [product_info_8.md]:\n",
      "\n",
      "- Brand: AlpineGear\n",
      "- Category: Tents\n",
      "- Price: $350\n",
      "- Waterproof: Provides reliable protection against rain and moisture.\n",
      "- Easy Setup: Simple and quick assembly process, thanks to color-coded poles and intuitive design.\n",
      "- Accommodation: Can accommodate two queen-sized air mattresses.\n",
      "\n",
      "Additionally, here is some warranty information about the tent:\n",
      "- Warranty Duration: The Alpine Explorer Tent is covered by a 2-year limited warranty from the date of purchase.\n",
      "- Warranty Coverage: The warranty covers manufacturing defects in materials and workmanship, including issues such as seam separation, zipper malfunction, or fabric defects.\n",
      "\n",
      "If you have any further questions or need assistance, please contact our customer support:\n",
      "- Customer Support Phone: +1-800-123-4567\n",
      "- Customer Support Email: support@example.com\n",
      "- Return Policy: Returns are accepted within 30 days of purchase, provided the tent is unused.\n",
      "\n",
      "For tent setup instructions, please follow these steps [product_info_1.md]:\n",
      "1. Select a suitable location by finding a level and clear area for pitching the tent, and removing any sharp objects or debris.\n",
      "2. Unpack and organize all the tent components on the ground.\n",
      "\n",
      "Please let me know if you need more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in message_history:\n",
    "    print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Azure OpenAI Service REST API\n",
    "\n",
    "This will require public access on Azure AI Search or a Microsoft managed private endpoint for private access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "import requests\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "open_ai_endpoint = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "open_ai_chat_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "open_ai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "chat_endpoint = f\"{open_ai_endpoint}/openai/deployments/{open_ai_chat_deployment}/chat/completions?api-version={open_ai_api_version}\"\n",
    "\n",
    "request_headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {access_token.token}\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_answer(message_history: list):\n",
    "    request_payload = {\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "                    \"index_name\": os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"),\n",
    "                    \"query_type\": \"vector_semantic_hybrid\",\n",
    "                    \"embedding_dependency\": {\n",
    "                        \"deployment_name\": os.getenv(\n",
    "                            \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"\n",
    "                        ),\n",
    "                        \"type\": \"deployment_name\",\n",
    "                    },\n",
    "                    \"fields_mapping\": {\"title_field\": \"title\", \"url_field\": \"path\"},\n",
    "                    \"authentication\": {\"type\": \"system_assigned_managed_identity\"},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": message_history,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        chat_endpoint,\n",
    "        headers=request_headers,\n",
    "        json=request_payload,\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_user_message = \"Which tent is the most waterproof?\"\n",
    "message_history = [\n",
    "    {\"role\": \"system\", \"content\": chat_system_message},\n",
    "    {\"role\": \"user\", \"content\": inital_user_message},\n",
    "]\n",
    "\n",
    "response = get_answer(message_history)\n",
    "message_history.append(\n",
    "    {\"role\": \"assistant\", \"content\": response[\"choices\"][0][\"message\"][\"content\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in message_history:\n",
    "    if message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "        print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_user_message = \"Tell me more about the Alpine Explorer Tent?\"\n",
    "message_history.append({\"role\": \"user\", \"content\": follow_up_user_message})\n",
    "\n",
    "response = get_answer(message_history)\n",
    "message_history.append(\n",
    "    {\"role\": \"assistant\", \"content\": response[\"choices\"][0][\"message\"][\"content\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in message_history:\n",
    "    if message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "        print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
