{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Scenario 1: Pull-Based Data Ingestion with Azure AI Search & Custom Retrieval-Augmented Generation (RAG) Pattern\n",
    "\n",
    "- **Populate Azure AI Search Index**: A pull-based approach is used to create a search index in Azure AI Search. A pull model uses indexers connecting to a supported data source, automatically uploading the data into your index. This is the recommended approach for data sources that are frequently updated.\n",
    "\n",
    "- **LLM Queries with Knowledge Base Integration**: A custom implementation for Retrieval Augmented Generation (RAG) will be used to chat with an LLM. This approach will be contrasted with an out-of-the box approach using the Azure OpenAI Service REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import sys\n",
    "\n",
    "# common setup\n",
    "dotenv.load_dotenv(\".env\")\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Populate Azure AI Search Index\n",
    "\n",
    "### Approach: Pull-Based Custom Client\n",
    "\n",
    "This approach will use the `CustomSearchClient` class defined in `search/custom_search_client_pull.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from search.custom_search_client import CustomSearchClient\n",
    "\n",
    "# Create search client\n",
    "search_client = CustomSearchClient(\n",
    "    search_endpoint=os.environ[\"AZURE_AI_SEARCH_ENDPOINT\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of variables to be used in templates\n",
    "template_variables = {\n",
    "    key: value for key, value in os.environ.items() if key.startswith((\"AZURE\"))\n",
    "}\n",
    "\n",
    "# Define template paths\n",
    "base_path = os.path.join(os.getcwd(), \"..\", \"search\", \"templates\")\n",
    "datasource_template_path = os.path.join(base_path, \"product-info\", \"datasource.json\")\n",
    "index_template_path = os.path.join(base_path, \"product-info\", \"index.json\")\n",
    "skillset_template_path = os.path.join(base_path, \"product-info\", \"skillset.json\")\n",
    "indexer_template_path = os.path.join(base_path, \"product-info\", \"indexer.json\")\n",
    "\n",
    "# List of search assets\n",
    "assets = [\n",
    "    {\n",
    "        \"type\": \"indexes\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_INDEX_NAME\"],\n",
    "        \"template_path\": index_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"datasources\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_DATASOURCE_NAME\"],\n",
    "        \"template_path\": datasource_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"skillsets\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_SKILLSET_NAME\"],\n",
    "        \"template_path\": skillset_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"indexers\",\n",
    "        \"name\": os.environ[\"AZURE_AI_SEARCH_INDEXER_NAME\"],\n",
    "        \"template_path\": indexer_template_path,\n",
    "        \"template_variables\": template_variables,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load search asset templates\n",
    "search_client.load_search_management_asset_templates(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the index\n",
    "index_response = search_client.create_search_management_asset(asset_type=\"indexes\")\n",
    "\n",
    "# Create the data source\n",
    "datasource_response = search_client.create_search_management_asset(\n",
    "    asset_type=\"datasources\"\n",
    ")\n",
    "\n",
    "# Create skillset to enhance the indexer\n",
    "skillset_response = search_client.create_search_management_asset(asset_type=\"skillsets\")\n",
    "\n",
    "# Create the indexer\n",
    "indexer_response = search_client.create_search_management_asset(asset_type=\"indexers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the indexer\n",
    "indexer_run_response = search_client.run_indexer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] Run the indexer with reset\n",
    "# indexer_run_reset_response = search_client.run_indexer(reset_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLM Queries with Knowledge Base Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import sys\n",
    "\n",
    "dotenv.load_dotenv(\".env\")\n",
    "sys.path.append(os.path.join(os.getcwd(), \"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Custom Client\n",
    "\n",
    "This approach will use the `CustomRetrievalAugmentedGenerationClient` class defined in `open_ai/custom_rag_client.py`. This will NOT require a Microsoft managed private endpoint for private access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Load configuration file\n",
    "system_prompt_configuration_file = \"../llms/system_messages.yml\"\n",
    "with open(system_prompt_configuration_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    configuration = yaml.safe_load(f)\n",
    "\n",
    "# Get system messages\n",
    "query_system_message = configuration.get(\"query_system_message\")\n",
    "chat_system_message = configuration.get(\"product_info_chat_system_message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llms.custom_rag_client import CustomRetrievalAugmentedGenerationClient\n",
    "\n",
    "# Create orchestration client\n",
    "rag_client = CustomRetrievalAugmentedGenerationClient(\n",
    "    open_ai_endpoint=os.getenv(\"AZURE_OPENAI_API_BASE\"),\n",
    "    open_ai_chat_deployment=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),\n",
    "    open_ai_embedding_deployment=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"),\n",
    "    search_endpoint=os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "    search_index_name=os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"),\n",
    "    query_system_message=query_system_message,\n",
    "    chat_system_message=chat_system_message,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = []\n",
    "message_history = rag_client.get_answer(\n",
    "    \"Which tent is the most waterproof?\", message_history=message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in message_history:\n",
    "    print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_history = rag_client.get_answer(\n",
    "    \"Tell me more about the Alpine Explorer Tent?\", message_history=message_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in message_history:\n",
    "    print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Azure OpenAI Service REST API\n",
    "\n",
    "This will require public access on Azure AI Search or a Microsoft managed private endpoint for private access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import DefaultAzureCredential\n",
    "import requests\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "access_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "open_ai_endpoint = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
    "open_ai_chat_deployment = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT\")\n",
    "open_ai_api_version = \"2024-02-01\"\n",
    "\n",
    "chat_endpoint = f\"{open_ai_endpoint}/openai/deployments/{open_ai_chat_deployment}/chat/completions?api-version={open_ai_api_version}\"\n",
    "\n",
    "request_headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {access_token.token}\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_answer(message_history: list):\n",
    "    request_payload = {\n",
    "        \"data_sources\": [\n",
    "            {\n",
    "                \"type\": \"azure_search\",\n",
    "                \"parameters\": {\n",
    "                    \"endpoint\": os.getenv(\"AZURE_AI_SEARCH_ENDPOINT\"),\n",
    "                    \"index_name\": os.getenv(\"AZURE_AI_SEARCH_INDEX_NAME\"),\n",
    "                    \"query_type\": \"vector_semantic_hybrid\",\n",
    "                    \"embedding_dependency\": {\n",
    "                        \"deployment_name\": os.getenv(\n",
    "                            \"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"\n",
    "                        ),\n",
    "                        \"type\": \"deployment_name\",\n",
    "                    },\n",
    "                    \"fields_mapping\": {\"title_field\": \"title\", \"url_field\": \"path\"},\n",
    "                    \"authentication\": {\"type\": \"system_assigned_managed_identity\"},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"messages\": message_history,\n",
    "    }\n",
    "\n",
    "    response = requests.post(\n",
    "        chat_endpoint,\n",
    "        headers=request_headers,\n",
    "        json=request_payload,\n",
    "    )\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inital_user_message = \"Which tent is the most waterproof?\"\n",
    "message_history = [\n",
    "    {\"role\": \"system\", \"content\": chat_system_message},\n",
    "    {\"role\": \"user\", \"content\": inital_user_message},\n",
    "]\n",
    "\n",
    "response = get_answer(message_history)\n",
    "message_history.append(\n",
    "    {\"role\": \"assistant\", \"content\": response[\"choices\"][0][\"message\"][\"content\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in message_history:\n",
    "    if message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "        print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_user_message = \"Tell me more about the Alpine Explorer Tent?\"\n",
    "message_history.append({\"role\": \"user\", \"content\": follow_up_user_message})\n",
    "\n",
    "response = get_answer(message_history)\n",
    "message_history.append(\n",
    "    {\"role\": \"assistant\", \"content\": response[\"choices\"][0][\"message\"][\"content\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in message_history:\n",
    "    if message[\"role\"] in [\"user\", \"assistant\"]:\n",
    "        print(f\"{message['role'].title()}: {message['content']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
